Core dumps
==========

This document explains the context and the available solutions to implement
core dumps in dune. It also explains in depth how the current solution works
and presents open issues for future work.

Information about the Linux kernel is based on Linux v4.4

## Objectives

There is currently no way to debug code running in Dune. The objective of this
project is to bring us one step closer to debuggability by allowing the 
developer to create core dumps of a process running in Dune.

It should be possible to create dumps programmatically, dumping core should not
crash the process nor should it alter the process's state in any way. With
these requirements, it becomes possible to create core dumps upon segmentation
fault simply by registering a custom segfault handler. It should also be
possible to dump core in response to Linux signals although this has not been
a consideration so far.


## Context

### Core dumps

A core dump in Linux is an elf file containing the process map, the contents of
each memory region, and CPU registers. The memory regions are stored as 
"program segments", the CPU state and memory map are stored as "notes" in the
elf file. A core file is considered like a "snapshot" of the running process.

A program such as `gdb` can make sense of the core file and, when the original
program binary with debugging symbols is available, enables the programmer to
recreate the state of the program as it was at the moment the core dump was
taken and inspect it.

#### Core dumps in Linux

In Linux, core dumps are generated by the function [do_coredump](http://elixir.free-electrons.com/Linux/v4.4.75/source/fs/coredump.c#L512)
in the kernel. This function handles high level operations such as figuring out
where to write the dump and delegates to [elf_format](http://elixir.free-electrons.com/Linux/v4.4.75/source/fs/binfmt_elf.c#L84)
for creating the actual dump file.

Core dumps are requested when a process crashes under certain circumstances
such as upon segmentation fault. They can also be requested as a default
action upon reception of some signals (see the manpage for `signal(7)`).

Whether the core dump is actually created and where it is written to
depends on system configuration as described in `core(5)`. Core files can be
written to disk or piped to some process depending on the value in
`/proc/sys/kern/core_pattern`. User and kernel limitations apply on the 
maximum size allowed for a core file. Typically, a user will have to run
`ulimit -c unlimited` before running a program to dump from.  
Other factors may affect the the way the core file is created so ill 
informed readers are invited to carefully read the `core(5)` manpage.

### Dune

Below is a _very_ succint description of the inner workings of Dune focused on
the issues that matter for the remainder of this document.

It is assumed that the reader is already familiar with the main paper 
_Dune: Safe User-level Access to Privileged CPU Features_.

#### Framework structure

Dune ships as two pieces of code: A kernel module and a userland API.

Application developers use libdune as the sole API for all Dune-related 
operations. libdune then talks to the kernel module by issuing ioctl
syscalls ont a special device created by the kernel module itself.
Therefore, we will consider `dune_dev_ioctl`, the ioctl handler in
`kern/core.c`, as the "entry point" of the kernel module for our purposes.

The most important ioctl is `DUNE_ENTER`. It is responsible for transitionning
the process into Dune mode. The handler for `DUNE_ENTER` calls into 
`vmx_launch`, which will be described later. It is worth noting that the
control flow breaks once `vmx_launch` is called and that it is likely that the
userspace code which lead into the ioctl syscall will resume **before** the
ioctl handler returns.

#### Virtual machine

`vmx.c` implements a sort of lightweight virtual machine based on intel's
VT-x instruction set extension. It is of highest importance to understand
how the virtual machine implemented in `vmx_launch` before going any further.

`vmx_launch` can be understood as an event loop running dune'd code at its
core. The events being anything that causes a VM_EXIT (any privileged 
operation we do not wish to expose to user code). Therefore, `vm_launch`
**does not return until after the process exits Dune mode**. The event
loop actually resumes the user process as if the ioctl to `DUNE_ENTER`
completed even though it did not and the process now runs in virtualized mode.

When the event loop eventually breaks, the ioctl does complete and the syscall
to ioctl(DUNE_ENTER) returns again, this time in Linux mode.
`libdune` encapsulates all of that black magic and makes sure the control flow
never magically jumps back to the user's call to `dune_enter`.

#### Kernel linking

The dune kernel module needs to call into kernel functions. The way Dune
"links" against those can be somewhat surprising.

The makefile calls a script to read the system map and extract addresses
of the required symbols. Those addresses are passed as global defines
to the appropriate compilation units. The C source then checks for the
existence of those defines and casts them into function pointers.


## Implementation

### Current solution

The current solution exploit the Linux kernel's `do_coredump` function by
directly calling into it from the dune kernel module. See below for a
discussion of alternative solutions and why this one was chosen.

A new ioctl opcode was added to allow the userspace to request a core dump.
Upon reception of such ioctl, the function `dune_dump_core` is called to 
attempt a core dump. Making such ioctl while not in dune mode results in
undefined behavior.

If we were to just call into `do_coredump` at that point, we would get the
correct memory segments but the register information would be wrong and
the core file would be unusable. To understand why, we need to take a step
back and figure out how the Linux kernel usually figures this out.

The memory is stored in `current->mm` and since we are still running a regular
Linux process, this information is still correct. The memory address 
translation that happens within the user code does not affect us here.

When `do_coredump` runs, the CPU state has already completely changed since
the process requested a core dump and it can therefore not be used as a note
in the core file. Instead, the kernel stores the CPU state when a process 
transitions from user mode to kernel mode such that it will be able to restore
the user process later on. This is what is used to provide CPU state
information in the core file.

Now, when a process running in Dune issues an ioctl to create a core dump,
it does not transition from userspace to kernel space, instead it goes from
virtualized mode to kernel mode through a `VM_EXIT`. In practice, we see the
call to `vmx_run_vcpu` in `vmx_launch` return and the Linux kernel did not see
any of this and did not get a chance to save the userspace CPU state.  
However, it is still necessary to save this somewhere and Dune is no exception.

What we need to do in `dune_dump_core` is copy the userspace CPU state from
Dune's storage into the kernel's. However, we must make sure to save and
restore the kernel storage or we will run into troubles when we leave Dune mode
and the initial call to dune_enter eventually completes.


### Discussion of alternatives

- **userspace core dump:** Writing a proper core dump library from scratch is 
extremely tedious and complex. There is little documentation on the subject.
[Google coredumper](https://code.google.com/archive/p/google-coredumper/) is
the only such project that can be found online but it has been abandonned a 
decade ago. Moreover, doing a core dump from kernel space is easier because
the CPU state is already stored.
- **porting Linux's core dumper to Dune:** This is a more realistic option 
because most of the code could be reused since Dune is a Linux process and has
most of the data structures required by the Linux core dumper.
However, this is still incredibly complex and would double the amount of code
in the Dune kernel module.

The current solution has its own drawbacks as well such as being highly reliant
on Linux's internal interfaces and feeling a bit hacky.
However, since we were able to fit a working prototype of that solution fits
within 150 lines of additional code split among the kernel module and libdune,
it is safe to say that it is not worse a solution than any other solution
considered.



## State of the project and further work

The current code is a working proof of concept of that implementation, it hasn't
been extensively tested and is incomplete. It is however already a big step 
towards Dune supporting core dumps and it could be used as-is to debug Dune 
applications.

Below is a list of important issues that need to be addressed:
- The current PoC shows that if we extract the stack pointer register from the
  Dune userspace CPU state storage and put it into Linux's, then the core dump
  we obtain shows the proper stack trace and stacked variables. Further work
  needs to be done to see if other registers need to be restored and to restore
  them.
- It will most likely not work with multithreaded applications and might even 
  cause a kernel panic in that situation.
- Complex use cases should be tested. The current test is too simple to cover
  interesting edge cases.
- Only the first call to `dune_core_dump` results in a core file. Subsequent
  calls seem to have no effect.
- The kernel code may not be properly protected against interruptions.
- A message is printed to stdout upon exit when core was dumped during the
  execution: `User defined signal 1 (core dumped)`.
- The `vcpu` variable is local to `vmx_launch` and it is currently exposed to
  the coredump execution flow through a global. This is definitely bad coding
  and may not behave properly when multiple processes run at the same time.
